{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L0wjLnS-KWo_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    # Sigmoid function\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate, num_iterations):\n",
        "    m = len(y)  # number of training examples\n",
        "    n = X.shape[1]  # number of features\n",
        "\n",
        "    # Initialize the parameters\n",
        "    theta = np.zeros((n, 1))\n",
        "    bias = 0\n",
        "\n",
        "    # Perform gradient descent\n",
        "    for iteration in range(num_iterations):\n",
        "        # Calculate linear logits\n",
        "        z = np.dot(X, theta) + bias\n",
        "\n",
        "        # Apply sigmoid activation function\n",
        "        y_pred = sigmoid(z)\n",
        "\n",
        "        # Calculate gradients\n",
        "        d_theta = (1 / m) * np.dot(X.T, (y_pred - y))\n",
        "        d_bias = (1 / m) * np.sum(y_pred - y)\n",
        "\n",
        "        # Update parameters\n",
        "        theta -= learning_rate * d_theta\n",
        "        bias -= learning_rate * d_bias\n",
        "\n",
        "    return theta, bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "euKDehTvKcu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}